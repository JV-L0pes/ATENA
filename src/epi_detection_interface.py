#!/usr/bin/env python3
"""
INTERFACE COMPLETA - Detec√ß√£o de EPIs em Imagens, V√≠deos e C√¢mera
Vers√£o funcional para POC comercial com m√∫ltiplas fontes
"""

import cv2
import numpy as np
import time
import tkinter as tk
from tkinter import filedialog, messagebox, ttk
from pathlib import Path
import threading
import queue
from PIL import Image, ImageTk

class EPIDetectionInterface:
    """Interface completa para detec√ß√£o de EPIs"""
    
    def __init__(self):
        """Inicializa interface"""
        self.model = None
        self.cap = None
        self.is_camera_running = False
        self.frame_queue = queue.Queue(maxsize=10)
        
        # Classes do modelo
        self.classes = ['helmet', 'no-helmet', 'no-vest', 'person', 'vest']
        
        # Cores para visualiza√ß√£o
        self.colors = {
            'person': (255, 255, 0),      # Amarelo
            'helmet': (0, 255, 0),        # Verde
            'vest': (0, 255, 0),          # Verde
            'no-helmet': (0, 0, 255),     # Vermelho
            'no-vest': (0, 0, 255)        # Vermelho
        }
        
        # Carrega modelo
        self.load_model()
        
        # Cria interface
        self.create_interface()
        
    def load_model(self):
        """Carrega modelo YOLOv5 otimizado"""
        try:
            # Modelo superior: epi_safe_fine_tuned
            weights_path = "yolov5/runs/train/epi_safe_fine_tuned/weights/best.pt"
            
            if not Path(weights_path).exists():
                print(f"‚ùå Modelo n√£o encontrado: {weights_path}")
                return False
            
            print(f"üöÄ Carregando modelo SUPERIOR: {weights_path}")
            print("üéØ Fine-tuned a partir de epi_detection_balanced_long_distance4")
            print("üìè Otimizado para longa dist√¢ncia (30m)")
            print("‚ö° Treinamento eficiente: 16 epochs")
            
            # Usa torch.hub diretamente
            import torch
            self.model = torch.hub.load('ultralytics/yolov5', 'custom', path=weights_path)
            
            # Configura√ß√µes ULTRA sens√≠veis para detectar pessoas
            self.model.conf = 0.20
            self.model.iou = 0.35
            
            print("‚úÖ Modelo SUPERIOR carregado com sucesso!")
            print("üèÜ mAP@0.5: 0.91 | Precision: 0.91 | Recall: 0.82")
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar modelo: {e}")
            return False
    
    def create_interface(self):
        """Cria interface gr√°fica"""
        self.root = tk.Tk()
        self.root.title("EPI Detection Interface - MVP")
        self.root.geometry("1200x800")
        self.root.configure(bg='#2b2b2b')
        
        # Estilo
        style = ttk.Style()
        style.theme_use('clam')
        
        # Frame principal
        main_frame = tk.Frame(self.root, bg='#2b2b2b')
        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
                # T√≠tulo
        title_label = tk.Label(main_frame, text="üèÜ DETEC√á√ÉO DE EPIs - MODELO SUPERIOR", 
                               font=('Arial', 20, 'bold'), fg='white', bg='#2b2b2b')
        title_label.pack(pady=(0, 20))
        
        # Subt√≠tulo com informa√ß√µes do modelo
        subtitle_label = tk.Label(main_frame, text="Fine-tuned para Longa Dist√¢ncia (30m) | mAP@0.5: 0.91", 
                                 font=('Arial', 12), fg='#4CAF50', bg='#2b2b2b')
        subtitle_label.pack(pady=(0, 20))
        
        # Frame de controles
        controls_frame = tk.Frame(main_frame, bg='#3b3b3b', relief='raised', bd=2)
        controls_frame.pack(fill=tk.X, pady=(0, 20))
        
        # Bot√µes de controle
        btn_frame = tk.Frame(controls_frame, bg='#3b3b3b')
        btn_frame.pack(pady=10)
        
        # Bot√£o Imagem
        self.img_btn = tk.Button(btn_frame, text="üì∏ DETECTAR IMAGEM", 
                                command=self.detect_image, 
                                font=('Arial', 12, 'bold'),
                                bg='#4CAF50', fg='white', 
                                relief='raised', bd=3, padx=20, pady=10)
        self.img_btn.pack(side=tk.LEFT, padx=10)
        
        # Bot√£o V√≠deo
        self.video_btn = tk.Button(btn_frame, text="üé• DETECTAR V√çDEO", 
                                  command=self.detect_video, 
                                  font=('Arial', 12, 'bold'),
                                  bg='#2196F3', fg='white', 
                                  relief='raised', bd=3, padx=20, pady=10)
        self.video_btn.pack(side=tk.LEFT, padx=10)
        
        # Bot√£o C√¢mera
        self.camera_btn = tk.Button(btn_frame, text="üìπ C√ÇMERA AO VIVO", 
                                   command=self.toggle_camera, 
                                   font=('Arial', 12, 'bold'),
                                   bg='#FF9800', fg='white', 
                                   relief='raised', bd=3, padx=20, pady=10)
        self.camera_btn.pack(side=tk.LEFT, padx=10)
        
        # Bot√£o Parar
        self.stop_btn = tk.Button(btn_frame, text="‚èπÔ∏è PARAR", 
                                 command=self.stop_camera, 
                                 font=('Arial', 12, 'bold'),
                                 bg='#f44336', fg='white', 
                                 relief='raised', bd=3, padx=20, pady=10)
        self.stop_btn.pack(side=tk.LEFT, padx=10)
        self.stop_btn.config(state=tk.DISABLED)
        
        # Frame de visualiza√ß√£o
        view_frame = tk.Frame(main_frame, bg='#1b1b1b', relief='sunken', bd=2)
        view_frame.pack(fill=tk.BOTH, expand=True)
        
        # Canvas para imagem/v√≠deo
        self.canvas = tk.Canvas(view_frame, bg='#1b1b1b', highlightthickness=0)
        self.canvas.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)
        
        # Frame de informa√ß√µes
        info_frame = tk.Frame(main_frame, bg='#3b3b3b', relief='raised', bd=2)
        info_frame.pack(fill=tk.X, pady=(20, 0))
        
        # Labels de informa√ß√£o
        self.status_label = tk.Label(info_frame, text="Status: Aguardando...", 
                                    font=('Arial', 12), fg='white', bg='#3b3b3b')
        self.status_label.pack(side=tk.LEFT, padx=10, pady=10)
        
        self.fps_label = tk.Label(info_frame, text="FPS: 0", 
                                 font=('Arial', 12), fg='white', bg='#3b3b3b')
        self.fps_label.pack(side=tk.LEFT, padx=10, pady=10)
        
        self.detection_label = tk.Label(info_frame, text="Detec√ß√µes: 0", 
                                       font=('Arial', 12), fg='white', bg='#3b3b3b')
        self.detection_label.pack(side=tk.LEFT, padx=10, pady=10)
        
        # Configura√ß√µes
        config_frame = tk.Frame(main_frame, bg='#3b3b3b', relief='raised', bd=2)
        config_frame.pack(fill=tk.X, pady=(20, 0))
        
        # Thresholds
        tk.Label(config_frame, text="Confidence:", font=('Arial', 10), 
                fg='white', bg='#3b3b3b').pack(side=tk.LEFT, padx=(10, 5), pady=10)
        
        self.conf_var = tk.DoubleVar(value=0.20)
        self.conf_scale = tk.Scale(config_frame, from_=0.1, to=0.9, resolution=0.05,
                                  orient=tk.HORIZONTAL, variable=self.conf_var,
                                  bg='#3b3b3b', fg='white', highlightthickness=0)
        self.conf_scale.pack(side=tk.LEFT, padx=(0, 20), pady=10)
        
        tk.Label(config_frame, text="IoU:", font=('Arial', 10), 
                fg='white', bg='#3b3b3b').pack(side=tk.LEFT, padx=(0, 5), pady=10)
        
        self.iou_var = tk.DoubleVar(value=0.35)
        self.iou_scale = tk.Scale(config_frame, from_=0.1, to=0.9, resolution=0.05,
                                 orient=tk.HORIZONTAL, variable=self.iou_var,
                                 bg='#3b3b3b', fg='white', highlightthickness=0)
        self.iou_scale.pack(side=tk.LEFT, padx=(0, 20), pady=10)
        
        # Bot√£o aplicar configura√ß√µes
        self.apply_btn = tk.Button(config_frame, text="‚úÖ APLICAR", 
                                  command=self.apply_config, 
                                  font=('Arial', 10, 'bold'),
                                  bg='#4CAF50', fg='white', 
                                  relief='raised', bd=2, padx=15, pady=5)
        self.apply_btn.pack(side=tk.LEFT, padx=(0, 10), pady=10)
        
        # Controle de otimiza√ß√£o de v√≠deo
        tk.Label(config_frame, text="V√≠deo:", font=('Arial', 10), 
                fg='white', bg='#3b3b3b').pack(side=tk.LEFT, padx=(20, 5), pady=10)
        
        self.video_opt_var = tk.IntVar(value=5)
        self.video_opt_scale = tk.Scale(config_frame, from_=1, to=15, resolution=1,
                                       orient=tk.HORIZONTAL, variable=self.video_opt_var,
                                       bg='#3b3b3b', fg='white', highlightthickness=0)
        self.video_opt_scale.pack(side=tk.LEFT, padx=(0, 20), pady=10)
        
        tk.Label(config_frame, text="frames", font=('Arial', 10), 
                fg='white', bg='#3b3b3b').pack(side=tk.LEFT, padx=(0, 10), pady=10)
        
        # Bind eventos
        self.conf_scale.bind("<ButtonRelease-1>", self.on_scale_change)
        self.iou_scale.bind("<ButtonRelease-1>", self.on_scale_change)
        
        # Inicializa vari√°veis
        self.current_image = None
        self.current_detections = []
        self.frame_count = 0
        self.start_time = time.time()
        
        # Vari√°veis de v√≠deo
        self.is_video_playing = False
        self.video_paused = False
        self.current_frame = 0
        
        # Sistema de tracking temporal para pessoas
        self.person_tracks = {}  # {track_id: {bbox, confidence, frames_seen}}
        self.next_track_id = 0
        
        # Atualiza interface
        self.update_interface()
    
    def on_scale_change(self, event):
        """Atualiza configura√ß√µes quando sliders mudam"""
        if self.model:
            self.model.conf = self.conf_var.get()
            self.model.iou = self.iou_var.get()
    
    def apply_config(self):
        """Aplica configura√ß√µes ao modelo"""
        if self.model:
            self.model.conf = self.conf_var.get()
            self.model.iou = self.iou_var.get()
            messagebox.showinfo("Configura√ß√£o", "Configura√ß√µes aplicadas com sucesso!")
    
    def detect_image(self):
        """Detecta EPIs em imagem"""
        if not self.model:
            messagebox.showerror("Erro", "Modelo n√£o carregado!")
            return
        
        # Abre di√°logo de arquivo
        file_path = filedialog.askopenfilename(
            title="Selecione uma imagem",
            filetypes=[
                ("Imagens", "*.jpg *.jpeg *.png *.bmp *.tiff"),
                ("Todos os arquivos", "*.*")
            ]
        )
        
        if not file_path:
            return
        
        try:
            # Carrega imagem
            image = cv2.imread(file_path)
            if image is None:
                messagebox.showerror("Erro", "N√£o foi poss√≠vel carregar a imagem!")
                return
            
            # Detecta objetos
            detections = self.detect_objects(image)
            
            # Valida EPIs
            validated_people = self.validate_epis(detections)
            
            # Desenha resultados
            result_image = self.draw_results(image, detections, validated_people)
            
            # Converte para Tkinter
            self.display_image(result_image)
            
            # Atualiza status
            self.status_label.config(text=f"Status: Imagem processada - {len(detections)} detec√ß√µes")
            self.detection_label.config(text=f"Detec√ß√µes: {len(detections)}")
            
            # Salva resultado
            self.save_result(result_image, "image")
            
        except Exception as e:
            messagebox.showerror("Erro", f"Erro ao processar imagem: {str(e)}")
    
    def detect_video(self):
        """Detecta EPIs em v√≠deo"""
        if not self.model:
            messagebox.showerror("Erro", "Modelo n√£o carregado!")
            return
        
        # Abre di√°logo de arquivo
        file_path = filedialog.askopenfilename(
            title="Selecione um v√≠deo",
            filetypes=[
                ("V√≠deos", "*.mp4 *.avi *.mov *.mkv *.wmv"),
                ("Todos os arquivos", "*.*")
            ]
        )
        
        if not file_path:
            return
        
        try:
            # Abre v√≠deo
            cap = cv2.VideoCapture(file_path)
            if not cap.isOpened():
                messagebox.showerror("Erro", "N√£o foi poss√≠vel abrir o v√≠deo!")
                return
            
            # Processa v√≠deo
            self.process_video(cap, file_path)
            
        except Exception as e:
            messagebox.showerror("Erro", f"Erro ao processar v√≠deo: {str(e)}")
    
    def process_video(self, cap, file_path):
        """Reproduz v√≠deo na velocidade nativa com detec√ß√£o em tempo real"""
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Configura√ß√µes de reprodu√ß√£o
        self.is_video_playing = True
        self.video_paused = False
        self.current_frame = 0
        self.video_cap = cap  # Guarda refer√™ncia para controles
        self.video_fps = fps  # Guarda FPS para usar no loop
        
        # Cria janela de v√≠deo com controles
        self.create_video_window(cap, file_path)
        
        # Inicia reprodu√ß√£o em thread separada
        self.video_thread = threading.Thread(target=self.video_loop, daemon=True)
        self.video_thread.start()
    
    def video_loop(self):
        """Loop de v√≠deo em thread separada otimizado para 30 FPS"""
        try:
            # For√ßa 30 FPS para reprodu√ß√£o suave
            target_fps = 30
            frame_delay = 1.0 / target_fps
            
            while self.is_video_playing and self.video_cap and self.video_cap.isOpened():
                if not self.video_paused:
                    ret, frame = self.video_cap.read()
                    if not ret:
                        break
                    
                    # Detecta objetos em tempo real
                    detections = self.detect_objects(frame)
                    validated_people = self.validate_epis(detections)
                    
                    # Desenha resultados com overlay
                    result_frame = self.draw_results(frame, detections, validated_people)
                    
                    # Atualiza interface na thread principal
                    self.root.after(0, self.update_video_frame, result_frame, detections, validated_people)
                    
                    # Controle de velocidade fixa em 30 FPS
                    time.sleep(frame_delay)
                    
                    self.current_frame += 1
                    
                    # Permite cancelar
                    if not self.root.winfo_exists():
                        break
                else:
                    # Pausado - aguarda
                    time.sleep(0.1)
            
        except Exception as e:
            print(f"‚ùå Erro no loop de v√≠deo: {e}")
            # Corrige erro de escopo na lambda
            error_msg = str(e)
            self.root.after(0, lambda: messagebox.showerror("Erro", f"Erro ao reproduzir v√≠deo: {error_msg}"))
        finally:
            if self.video_cap:
                self.video_cap.release()
            self.is_video_playing = False
    
    def update_video_frame(self, frame, detections, validated_people):
        """Atualiza frame do v√≠deo na interface principal"""
        try:
            # Exibe frame com detec√ß√µes
            self.display_video_frame(frame)
            
            # Atualiza status
            self.update_video_status(frame, detections, validated_people)
        except Exception as e:
            print(f"‚ùå Erro ao atualizar frame: {e}")
    
    def create_video_window(self, cap, file_path):
        """Cria janela de v√≠deo com controles simplificados"""
        # Frame de controles de v√≠deo
        self.video_controls = tk.Frame(self.root, bg='#3b3b3b', relief='raised', bd=2)
        self.video_controls.pack(fill=tk.X, pady=(10, 0))
        
        # Bot√µes de controle simplificados
        self.play_pause_btn = tk.Button(self.video_controls, text="‚è∏Ô∏è PAUSAR", 
                                       command=self.toggle_video_pause, 
                                       font=('Arial', 12, 'bold'),
                                       bg='#FF9800', fg='white', 
                                       relief='raised', bd=2, padx=20, pady=8)
        self.play_pause_btn.pack(side=tk.LEFT, padx=20, pady=8)
        
        # Label de status simples
        self.video_status_label = tk.Label(self.video_controls, text="üé• V√≠deo em reprodu√ß√£o", 
                                          font=('Arial', 12), fg='white', bg='#3b3b3b')
        self.video_status_label.pack(side=tk.LEFT, padx=20, pady=8)
    
    def toggle_video_pause(self):
        """Alterna pausa do v√≠deo"""
        self.video_paused = not self.video_paused
        if self.video_paused:
            self.play_pause_btn.config(text="‚ñ∂Ô∏è CONTINUAR", bg='#4CAF50')
        else:
            self.play_pause_btn.config(text="‚è∏Ô∏è PAUSAR", bg='#FF9800')
    
    def stop_video(self):
        """Para reprodu√ß√£o do v√≠deo"""
        self.is_video_playing = False
        self.video_paused = False
        
        # Remove controles de v√≠deo da interface
        try:
            if hasattr(self, 'video_controls'):
                self.video_controls.destroy()
        except:
            pass
    
    def seek_video(self, frame_num):
        """Pula para frame espec√≠fico (mantido para compatibilidade)"""
        pass  # Removido - controles simplificados
    
    def display_video_frame(self, frame):
        """Exibe frame do v√≠deo com detec√ß√µes"""
        self.display_image(frame)
    
    def update_video_status(self, frame, detections, validated_people):
        """Atualiza status do v√≠deo simplificado"""
        self.status_label.config(text=f"Status: Reproduzindo v√≠deo - Frame {self.current_frame}")
        self.detection_label.config(text=f"Detec√ß√µes: {len(detections)}")
    
    def toggle_camera(self):
        """Alterna c√¢mera ao vivo"""
        if not self.is_camera_running:
            self.start_camera()
        else:
            self.stop_camera()
    
    def start_camera(self):
        """Inicia c√¢mera ao vivo"""
        try:
            self.cap = cv2.VideoCapture(0)
            if not self.cap.isOpened():
                messagebox.showerror("Erro", "N√£o foi poss√≠vel abrir a c√¢mera!")
                return
            
            # Configura√ß√µes b√°sicas
            self.cap.set(cv2.CAP_PROP_FPS, 30)
            
            self.is_camera_running = True
            self.camera_btn.config(text="üìπ PARAR C√ÇMERA", bg='#f44336')
            self.stop_btn.config(state=tk.NORMAL)
            
            # Inicia thread de c√¢mera
            self.camera_thread = threading.Thread(target=self.camera_loop, daemon=True)
            self.camera_thread.start()
            
            self.status_label.config(text="Status: C√¢mera ativa")
            
        except Exception as e:
            messagebox.showerror("Erro", f"Erro ao iniciar c√¢mera: {str(e)}")
    
    def stop_camera(self):
        """Para c√¢mera ao vivo"""
        self.is_camera_running = False
        
        if self.cap:
            self.cap.release()
            self.cap = None
        
        try:
            if self.root.winfo_exists():
                self.camera_btn.config(text="üìπ C√ÇMERA AO VIVO", bg='#FF9800')
                self.stop_btn.config(state=tk.DISABLED)
                self.status_label.config(text="Status: C√¢mera parada")
        except:
            pass
    
    def camera_loop(self):
        """Loop principal da c√¢mera"""
        while self.is_camera_running:
            if self.cap and self.cap.isOpened():
                ret, frame = self.cap.read()
                if ret:
                    # Detecta objetos
                    detections = self.detect_objects(frame)
                    
                    # Valida EPIs
                    validated_people = self.validate_epis(detections)
                    
                    # Desenha resultados
                    result_frame = self.draw_results(frame, detections, validated_people)
                    
                    # Adiciona √† fila para exibi√ß√£o
                    if not self.frame_queue.full():
                        self.frame_queue.put(result_frame)
                    
                    # Atualiza estat√≠sticas
                    self.frame_count += 1
                    elapsed_time = time.time() - self.start_time
                    fps = self.frame_count / elapsed_time if elapsed_time > 0 else 0
                    
                    # Atualiza interface na thread principal
                    self.root.after(0, self.update_camera_stats, fps, len(detections))
            
            time.sleep(0.033)  # ~30 FPS
    
    def update_camera_stats(self, fps, detections):
        """Atualiza estat√≠sticas da c√¢mera na interface"""
        try:
            if self.root.winfo_exists():
                self.fps_label.config(text=f"FPS: {fps:.1f}")
                self.detection_label.config(text=f"Detec√ß√µes: {detections}")
                
                # Atualiza imagem se houver frame na fila
                try:
                    if not self.frame_queue.empty():
                        frame = self.frame_queue.get_nowait()
                        self.display_image(frame)
                except queue.Empty:
                    pass
        except:
            pass
    
    def detect_objects(self, frame):
        """Detec√ß√£o de objetos YOLOv5 com filtros anti-falso positivo"""
        if self.model is None:
            return []
        
        try:
            # Redimensiona para 640x640 (padr√£o YOLO)
            frame_resized = cv2.resize(frame, (640, 640))
            
            # Detec√ß√£o YOLO
            results = self.model(frame_resized)
            
            # Converte coordenadas para frame original
            h, w = frame.shape[:2]
            detections = []
            
            # Acessa resultados corretamente para YOLOv5
            if hasattr(results, 'xyxy') and len(results.xyxy) > 0:
                # Formato YOLOv5: [x1, y1, x2, y2, conf, cls]
                detections_array = results.xyxy[0].cpu().numpy()
                
                for detection in detections_array:
                    x1, y1, x2, y2, conf, cls = detection
                    
                    # Converte para coordenadas do frame original
                    x1 = int(x1 * w / 640)
                    y1 = int(y1 * h / 640)
                    x2 = int(x2 * w / 640)
                    y2 = int(y2 * h / 640)
                    
                    # Verifica se a classe est√° no range v√°lido
                    if 0 <= int(cls) < len(self.classes):
                        # üîç FILTRO ADICIONAL: Valida√ß√£o de confian√ßa por classe
                        if not self.is_valid_detection_confidence(cls, conf):
                            continue
                        
                        detections.append({
                            'bbox': [x1, y1, x2, y2],
                            'confidence': float(conf),
                            'class_id': int(cls),
                            'class_name': self.classes[int(cls)]
                        })
            
            return detections
            
        except Exception as e:
            print(f"‚ùå Erro na detec√ß√£o: {e}")
            return []
    
    def validate_epis(self, detections):
        """Valida√ß√£o inteligente de EPIs com tracking temporal"""
        people = [d for d in detections if d['class_name'] == 'person']
        helmets = [d for d in detections if d['class_name'] == 'helmet']
        vests = [d for d in detections if d['class_name'] == 'vest']
        
        validated_people = []
        
        for person in people:
            px1, py1, px2, py2 = person['bbox']
            person_width = px2 - px1
            person_height = py2 - py1
            
            # üîç VALIDA√á√ÉO INTELIGENTE: Combina an√°lise de forma + tracking temporal
            is_human_by_form = self.is_likely_human(person_width, person_height, person)
            
            # Adiciona tracking temporal
            track_id = self.track_person_temporally(person)
            is_human_by_tracking = self.is_tracked_person_likely_human(track_id)
            
            # üöó VALIDA√á√ÉO DE MOVIMENTO: Detecta ve√≠culos vs pessoas
            is_human_by_movement = self._validate_human_movement(track_id)
            
            # ‚úÖ DECIS√ÉO FINAL: Todas as valida√ß√µes devem passar para ser humano
            if not (is_human_by_form and is_human_by_tracking and is_human_by_movement):
                print(f"‚ùå Objeto descartado: n√£o √© humano (w:{person_width}, h:{person_height}, track:{track_id})")
                print(f"   - Forma: {is_human_by_form}, Tracking: {is_human_by_tracking}, Movimento: {is_human_by_movement}")
                continue
            
            person_center = ((px1 + px2) // 2, (py1 + py2) // 2)
            
            # Procura capacete pr√≥ximo da cabe√ßa
            helmet_found = False
            for helmet in helmets:
                hx1, hy1, hx2, hy2 = helmet['bbox']
                helmet_center = ((hx1 + hx2) // 2, (hy1 + hy2) // 2)
                
                # Dist√¢ncia simples
                distance = np.sqrt((person_center[0] - helmet_center[0])**2 + 
                                 (person_center[1] - helmet_center[1])**2)
                
                # Capacete no topo da pessoa
                if distance < 100 and hy2 < py2 * 0.4:
                    helmet_found = True
                    break
            
            # Procura colete pr√≥ximo do torso
            vest_found = False
            for vest in vests:
                vx1, vy1, vx2, vy2 = vest['bbox']
                vest_center = ((vx1 + vx2) // 2, (vy1 + vy2) // 2)
                
                # Dist√¢ncia simples
                distance = np.sqrt((person_center[0] - vest_center[0])**2 + 
                                 (person_center[1] - vest_center[1])**2)
                
                # Colete no meio da pessoa
                if distance < 120 and vy1 > py1 * 0.3 and vy2 < py2 * 0.8:
                    vest_found = True
                    break
            
            validated_people.append({
                'person': person,
                'helmet_found': helmet_found,
                'vest_found': vest_found,
                'track_id': track_id
            })
        
        return validated_people
    

    
    def is_valid_detection_confidence(self, class_id, confidence):
        """Valida se a confian√ßa √© adequada para cada classe"""
        # Thresholds ULTRA sens√≠veis para detectar pessoas em qualquer situa√ß√£o
        confidence_thresholds = {
            0: 0.20,  # helmet - ultra sens√≠vel
            1: 0.25,  # no-helmet - ultra sens√≠vel
            2: 0.25,  # no-vest - ultra sens√≠vel
            3: 0.20,  # person - ultra sens√≠vel (detecta TODAS as pessoas)
            4: 0.20   # vest - ultra sens√≠vel
        }
        threshold = confidence_thresholds.get(int(class_id), 0.15)
        return confidence >= threshold
    
    def is_likely_human(self, width, height, person_detection):
        """Valida√ß√£o inteligente: diferencia pessoas de objetos usando heur√≠sticas avan√ßadas"""
        if height <= 0 or width <= 0:
            return False
        
        # üéØ ESTRAT√âGIA PRINCIPAL: CONFIAN√áA ALTA = MAIS PROV√ÅVEL SER HUMANO
        confidence = person_detection['confidence']
        
        print(f"üîç VALIDA√á√ÉO: {width}x{height}px, conf: {confidence:.2f}")
        
        # ‚úÖ REGRA 1: Se confian√ßa √© alta (> 0.5), valida com filtros rigorosos
        if confidence > 0.5:
            print(f"üéØ REGRA 1: Confian√ßa alta ({confidence:.2f}) - valida√ß√£o rigorosa")
            return self._strict_human_validation(width, height, person_detection)
        
        # ‚úÖ REGRA 2: Se confian√ßa √© m√©dia-alta (> 0.4), valida com filtros m√©dios
        if confidence > 0.4:
            print(f"üéØ REGRA 2: Confian√ßa m√©dia-alta ({confidence:.2f}) - valida√ß√£o m√©dia")
            return self._medium_human_validation(width, height, person_detection)
        
        # ‚úÖ REGRA 3: Se confian√ßa √© m√©dia (> 0.25), usa filtros rigorosos
        if confidence > 0.25:  # Ajustado para 0.25
            print(f"üéØ REGRA 3: Confian√ßa m√©dia ({confidence:.2f}) - valida√ß√£o rigorosa")
            return self._strict_human_validation(width, height, person_detection)
        
        # ‚ùå REGRA 4: Confian√ßa muito baixa (< 0.25) = descarta
        print(f"‚ùå REGRA 4: Confian√ßa muito baixa ({confidence:.2f}) - REJEITADO")
        return False
    
    def _strict_human_validation(self, width, height, person_detection):
        """Valida√ß√£o rigorosa para diferenciar pessoas de motos/ve√≠culos"""
        
        print(f"üîç VALIDA√á√ÉO RIGOROSA: {width}x{height}px")
        
        # üö´ FILTROS ANTI-MOTO/VE√çCULO RIGOROSOS:
        
        # 1. Altura m√≠nima para pessoa (1.40m relativo)
        min_height_px = 140
        if height < min_height_px:
            print(f"‚ùå Objeto muito baixo para ser pessoa: {height}px < {min_height_px}px")
            return False
        
        # 2. Propor√ß√µes anat√¥micas humanas RIGOROSAS
        aspect_ratio = height / width
        print(f"üìê Aspect ratio: {aspect_ratio:.2f} (deve ser >= 1.8)")
        
        # Pessoas devem ser MUITO mais altas que largas
        if aspect_ratio < 1.8:  # Mais rigoroso que antes
            print(f"‚ùå Objeto muito largo para ser pessoa: ratio {aspect_ratio:.2f} < 1.8")
            return False
        
        if aspect_ratio > 6.0:  # Menos rigoroso para postes
            print(f"‚ùå Objeto muito estreito para ser pessoa: ratio {aspect_ratio:.2f}")
            return False
        
        # 3. Valida√ß√£o de forma corporal RIGOROSA
        print(f"üîç Verificando forma de ve√≠culo...")
        if self._is_vehicle_like_shape(width, height, person_detection):
            print(f"‚ùå Forma muito similar a ve√≠culo")
            return False
        
        # 4. Valida√ß√£o de simetria (motos s√£o mais sim√©tricas)
        print(f"üîç Verificando simetria...")
        if self._is_too_symmetric(width, height, person_detection):
            print(f"‚ùå Objeto muito sim√©trico (poss√≠vel ve√≠culo)")
            return False
        
        # 5. Tamanhos extremos
        if width < 30 or height < 50:
            print(f"‚ùå Objeto muito pequeno: {width}x{height}")
            return False
        
        if width > 1500 or height > 3000:
            print(f"‚ùå Objeto muito grande: {width}x{height}")
            return False
        
        print(f"‚úÖ VALIDA√á√ÉO RIGOROSA PASSOU - provavelmente pessoa")
        return True
    
    def _medium_human_validation(self, width, height, person_detection):
        """Valida√ß√£o m√©dia para confian√ßa m√©dia-alta"""
        aspect_ratio = height / width
        
        # Mais flex√≠vel, mas ainda rigoroso
        if aspect_ratio < 1.6:  # Menos rigoroso que strict
            print(f"‚ùå Objeto muito largo para ser pessoa: ratio {aspect_ratio:.2f} < 1.6")
            return False
        
        if aspect_ratio > 7.0:
            print(f"‚ùå Objeto muito estreito para ser pessoa: ratio {aspect_ratio:.2f}")
            return False
        
        # Valida√ß√£o b√°sica de forma
        if self._is_obviously_vehicle(width, height, person_detection):
            print(f"‚ùå Obviamente ve√≠culo")
            return False
        
        return True
    
    def _is_vehicle_like_shape(self, width, height, person_detection):
        """Detec√ß√£o avan√ßada de formas de ve√≠culos"""
        width_height_ratio = width / height
        print(f"üîç Verificando forma: width/height = {width_height_ratio:.2f}")
        
        # üèçÔ∏è DETEC√á√ÉO ESPEC√çFICA DE MOTOS:
        # Motos t√™m propor√ß√µes caracter√≠sticas: largura similar √† altura
        if 0.7 <= width_height_ratio <= 1.3:  # Quase quadrado
            print(f"üèçÔ∏è FORMA QUASE QUADRADA (ratio {width_height_ratio:.2f}) - POSS√çVEL MOTO!")
            return True
        
        # üöó VE√çCULOS LARGOS:
        if width_height_ratio > 1.5:  # Muito mais largo que alto
            print(f"üöó MUITO LARGO (ratio {width_height_ratio:.2f}) - POSS√çVEL VE√çCULO!")
            return True
        
        # üî≤ FORMAS RETANGULARES PERFEITAS:
        if 0.8 <= width_height_ratio <= 1.2:  # Muito retangular
            print(f"üî≤ FORMA MUITO RETANGULAR (ratio {width_height_ratio:.2f}) - POSS√çVEL VE√çCULO!")
            return True
        
        print(f"‚úÖ Forma n√£o parece ve√≠culo (ratio {width_height_ratio:.2f})")
        return False
    
    def _is_obviously_vehicle(self, width, height, person_detection):
        """Detec√ß√£o de ve√≠culos √≥bvios"""
        width_height_ratio = width / height
        
        # Casos √≥bvios
        if width_height_ratio > 2.0:  # Extremamente largo
            return True
        
        if 0.6 <= width_height_ratio <= 1.4:  # Quase quadrado
            return True
        
        return False
    
    def _is_too_symmetric(self, width, height, person_detection):
        """Detecta se o objeto √© muito sim√©trico (t√≠pico de ve√≠culos)"""
        # Motos e ve√≠culos s√£o mais sim√©tricos que pessoas
        # Pessoas t√™m varia√ß√£o natural de largura ao longo da altura
        
        # Se a largura √© muito consistente, pode ser ve√≠culo
        # Esta √© uma valida√ß√£o adicional para casos duvidosos
        
        # Para implementa√ß√£o futura: an√°lise de densidade de pixels
        # Por enquanto, usa propor√ß√µes como proxy
        width_height_ratio = width / height
        
        # Objetos muito sim√©tricos tendem a ter propor√ß√µes pr√≥ximas de 1:1
        if 0.7 <= width_height_ratio <= 1.3:
            return True
        
        return False
    

    
    def track_person_temporally(self, person_detection):
        """Sistema de tracking temporal para melhorar detec√ß√£o de pessoas"""
        bbox = person_detection['bbox']
        confidence = person_detection['confidence']
        person_center = ((bbox[0] + bbox[2]) // 2, (bbox[1] + bbox[3]) // 2)
        
        # Procura por track existente pr√≥ximo
        best_track_id = None
        best_distance = float('inf')
        
        for track_id, track_data in self.person_tracks.items():
            track_bbox = track_data['bbox']
            
            # Calcula dist√¢ncia entre centroides
            track_center = ((track_bbox[0] + track_bbox[2]) // 2, (track_bbox[1] + track_bbox[3]) // 2)
            
            distance = np.sqrt((person_center[0] - track_center[0])**2 + 
                             (person_center[1] - track_center[1])**2)
            
            if distance < 100 and distance < best_distance:  # Threshold de 100px
                best_distance = distance
                best_track_id = track_id
        
        if best_track_id is not None:
            # Atualiza track existente
            track_data = self.person_tracks[best_track_id]
            track_data.update({
                'bbox': bbox,
                'confidence': confidence,
                'frames_seen': track_data['frames_seen'] + 1
            })
            
            # Adiciona ao hist√≥rico de movimento
            if 'movement_history' not in track_data:
                track_data['movement_history'] = []
            
            track_data['movement_history'].append({
                'center': person_center,
                'frame': self.frame_count,
                'bbox': bbox
            })
            
            # Mant√©m apenas os √∫ltimos 10 frames para an√°lise
            if len(track_data['movement_history']) > 10:
                track_data['movement_history'] = track_data['movement_history'][-10:]
            
            return best_track_id
        else:
            # Cria novo track
            track_id = self.next_track_id
            self.next_track_id += 1
            
            self.person_tracks[track_id] = {
                'bbox': bbox,
                'confidence': confidence,
                'frames_seen': 1,
                'movement_history': [{
                    'center': person_center,
                    'frame': self.frame_count,
                    'bbox': bbox
                }]
            }
            return track_id
    
    def is_tracked_person_likely_human(self, track_id):
        """Valida se uma pessoa trackeada √© provavelmente humana"""
        if track_id not in self.person_tracks:
            return False
        
        track_data = self.person_tracks[track_id]
        frames_seen = track_data['frames_seen']
        avg_confidence = track_data['confidence']
        
        # ‚úÖ REGRA 1: Se foi vista em muitos frames, provavelmente √© humana
        if frames_seen >= 3:
            return True
        
        # ‚úÖ REGRA 2: Se confian√ßa m√©dia √© alta, provavelmente √© humana
        if avg_confidence > 0.4:
            return True
        
        # ‚úÖ REGRA 3: Se foi vista em alguns frames com confian√ßa m√©dia
        if frames_seen >= 2 and avg_confidence > 0.3:
            return True
        
        return False
    
    def _validate_human_movement(self, track_id):
        """Valida se o movimento √© t√≠pico de pessoa vs ve√≠culo"""
        if track_id not in self.person_tracks:
            return False
        
        track_data = self.person_tracks[track_id]
        
        # Se n√£o tem hist√≥rico suficiente, n√£o pode validar
        if 'movement_history' not in track_data or len(track_data['movement_history']) < 3:
            return True  # Assume que √© humano se n√£o pode validar
        
        movement_history = track_data['movement_history']
        
        # Calcula varia√ß√£o de movimento
        # Pessoas t√™m movimento mais org√¢nico, ve√≠culos mais linear
        movements = []
        for i in range(1, len(movement_history)):
            prev_center = movement_history[i-1]['center']
            curr_center = movement_history[i]['center']
            
            dx = curr_center[0] - prev_center[0]
            dy = curr_center[1] - prev_center[1]
            movement = np.sqrt(dx*dx + dy*dy)
            movements.append(movement)
        
        if len(movements) < 2:
            return True
        
        # Calcula varia√ß√£o do movimento
        movement_std = np.std(movements)
        movement_mean = np.mean(movements)
        
        # Se movimento √© muito consistente (linear), pode ser ve√≠culo
        if movement_mean > 0 and movement_std / movement_mean < 0.3:
            print(f"‚ö†Ô∏è Movimento muito linear - poss√≠vel ve√≠culo (std/mean: {movement_std/movement_mean:.2f})")
            return False
        
        return True

    def draw_results(self, frame, detections, validated_people):
        """Desenha resultados na imagem"""
        frame_copy = frame.copy()
        
        # Desenha todas as detec√ß√µes
        for detection in detections:
            x1, y1, x2, y2 = detection['bbox']
            class_name = detection['class_name']
            conf = detection['confidence']
            
            # Cor baseada na classe
            color = self.colors.get(class_name, (128, 128, 128))
            
            # Desenha bounding box
            cv2.rectangle(frame_copy, (x1, y1), (x2, y2), color, 2)
            
            # Label
            label = f"{class_name}: {conf:.2f}"
            cv2.putText(frame_copy, label, (x1, y1 - 10), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
        
        # Desenha informa√ß√µes das pessoas
        for person_data in validated_people:
            person = person_data['person']
            px1, py1, px2, py2 = person['bbox']
            
            # Info da pessoa
            info_text = f"PESSOA"
            if person_data['helmet_found']:
                info_text += " + CAPACETE"
            if person_data['vest_found']:
                info_text += " + COLETE"
            
            cv2.putText(frame_copy, info_text, (px1, py2 + 25), 
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        return frame_copy
    
    def display_image(self, image):
        """Exibe imagem no canvas"""
        # Redimensiona para caber no canvas
        canvas_width = self.canvas.winfo_width()
        canvas_height = self.canvas.winfo_height()
        
        if canvas_width > 1 and canvas_height > 1:
            # Converte BGR para RGB
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Redimensiona mantendo propor√ß√£o
            h, w = image_rgb.shape[:2]
            scale = min(canvas_width / w, canvas_height / h)
            new_w, new_h = int(w * scale), int(h * scale)
            
            image_resized = cv2.resize(image_rgb, (new_w, new_h))
            
            # Converte para PIL
            pil_image = Image.fromarray(image_resized)
            self.current_image = ImageTk.PhotoImage(pil_image)
            
            # Limpa canvas e exibe nova imagem
            self.canvas.delete("all")
            self.canvas.create_image(canvas_width//2, canvas_height//2, 
                                   image=self.current_image, anchor=tk.CENTER)
    
    def save_result(self, image, source_type):
        """Salva resultado da detec√ß√£o"""
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f"epi_detection_{source_type}_{timestamp}.jpg"
        
        cv2.imwrite(filename, image)
        print(f"üì∏ Resultado salvo: {filename}")
    
    def update_interface(self):
        """Atualiza interface periodicamente"""
        try:
            if self.root.winfo_exists():
                self.root.after(100, self.update_interface)
        except:
            pass
    
    def run(self):
        """Executa interface"""
        try:
            self.root.mainloop()
        except KeyboardInterrupt:
            print("üõë Interface interrompida pelo usu√°rio")
        finally:
            self.stop_camera()

def main():
    print("üèÜ INTERFACE COMPLETA - DETEC√á√ÉO DE EPIs COM MODELO SUPERIOR")
    print("=" * 70)
    print("üì∏ Imagens, üé• V√≠deos, üìπ C√¢mera ao vivo")
    print("üéØ Modelo: epi_safe_fine_tuned (Fine-tuned para Longa Dist√¢ncia)")
    print("üìè Otimizado para 30m | mAP@0.5: 0.91 | Precision: 0.91")
    print("=" * 70)
    
    # Cria e executa interface
    interface = EPIDetectionInterface()
    interface.run()

if __name__ == "__main__":
    main()
